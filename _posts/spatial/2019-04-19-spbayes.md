---
layout: post
title: spBayes
date: 2019-04-19 14:49 -0600
categories: Research
tags: [spatial]
---

# Bayesian gaussian spatial regression models

###### Hierarchical linear mixed model framework

$$
p(\theta)\times N(\beta|\mu_{\beta},\Sigma_{\beta})\times N(\alpha|0,K(\theta))\times N(y|X\beta+Z(\theta)\alpha,D(\theta))
$$

- $X$ known $n\times p$, $p<n$
- $K(\theta)$: $r\times r$, $r\leq n$  **lower rank structure**
- $D(\theta)$: $n\times n$
- $\alpha \sim N(0,K(\theta))$

###### Bayesian inference is carried out by sampling from the posterior distribution of $$\{\beta,\alpha,\theta\}$$.

### Sampling the process parameters
MCMC:
- Gibbs Sampling
- Random walk Metropolis steps

###### How to achieve faster convergence?
- By integrating out $\beta$ and $\alpha$ from the model

1. sample from $p(\theta\|y) \propto p(\theta)\times N(y\|X\mu_{\beta},\Sigma_{y\|\theta})$,

$$
\Sigma_{y|\theta} = X\Sigma_{\beta}X^{\top} + Z(\theta)K(\theta)Z(\theta)^{\top} + D(\theta)
$$

Bottleneck: $Z(\theta)K(\theta)Z(\theta)^{\top}$

$$
\log p(\theta|y) = \text{const} + \log p(\theta) - \frac{1}{2}\log|\Sigma_{y|\theta}| - \frac{1}{2}Q(\theta)
$$

$$
Q(\theta) = (y-X\mu_{\beta})^{\top} \Sigma_{y|\theta}^{-1}(y-X\mu_{\beta})
$$

计算过程：

1.get lower triangular Cholesky factor  **$O(n^3/2)$**

$$
L = \texttt{chol}(\Sigma_{y|\theta})
$$

2.trsolve **$O(n^2)$**

$$
u = \texttt{trsolve}(L,y-X\mu_{\beta})
$$

3.Flops: **$2n$**

$$
Q(\theta) = u^{\top}u
$$

4.Flops: **$n$**

$$
\log|\Sigma_{y|\theta}| = 2\sum_{i=1}^n\log l_{ii}
$$

If $\beta$ is flat, i.e., $\Sigma_{\beta}^{-1} = 0$

$$
\log p(\theta|y) = \text{const} + \log p(\theta) - \frac{1}{2}\log|X^{\top}\Sigma_{y|\beta,\theta}X|- \frac{1}{2}\log|\Sigma_{y|\beta,\theta}| - \frac{1}{2}Q(\theta)
$$

$$
\Sigma_{y|\beta,\theta} = Z(\theta)K(\theta)Z(\theta)^{\top} + D(\theta)
$$

$$
Q(\theta) = y^{\top}\Sigma_{y|\beta,\theta}^{-1}y - b^{\top}(X^{\top}\Sigma_{y|\beta,\theta}X)^{-1}b
$$

$$
b = X^{\top}\Sigma_{y|\beta,\theta}^{-1}y
$$

计算过程：
1.get lower triangular Cholesky factor  **$O(n^3/2)$**

$$
L = \texttt{chol}(\Sigma_{y|\beta,\theta})
$$

2.trsolve **$O(n^2)$**

$$
[v:U] = \texttt{trsolve}(L,[y:X])
$$

3.

$$
W = \texttt{chol}(U^{\top}U)
$$

$$
b = U^{\top}v
$$

$$
\tilde{b} = \texttt{trsolve}(W,b)
$$

4. Flops **$n$**
$$
\log p(\theta) - \sum_{i=1}^p\log w_{i,i} - \sum_{i=1}^n\log l_{i,i} - \frac{1}{2}(v^\top v - \tilde{b}^{\top}\tilde{b})
$$


### Sampling the slope and the random effects

* Step 1: get marginal posterior samples $\theta$ from $p(\theta\|y)$
* Step 2: *composition Sampling*
  - M samples from $p(\theta\|y)$: $$\{\theta^{(1)},\theta^{(2)},\ldots,\theta^{(M)}\}$$.
  - drawing $\beta^{(k)} \sim p(\beta\|\theta^{(k)},y)$ and $\alpha^{(k)} \sim p(\alpha\|\theta^{(k)},y)$

#### Slope $\beta$

$$
\beta|\theta,y \sim N_p(Bb,B)
$$

$$
b=\Sigma_{\beta}^{-1}\mu_{\beta} + X^{\top}\Sigma_{y|\beta,\theta}^{-1}y
$$

$$
B = (\Sigma_{\beta}^{-1} + X^{\top}\Sigma_{y|\beta,\theta}^{-1}X)^{-1}
$$

- for current $\theta^{(k)}$, draw $\beta^{(k)}\sim N_p(Bb,B)$.
- $$L = \texttt{chol}(\Sigma_{y\|\beta,\theta^{(k)}})$$,
- $[v:U] = \texttt{trsolve}(L,[y:X])$

$$
\beta^{(k)} = \texttt{trsolve}(L_B^{\top},\texttt{trsolve}(L_B,b)) + \texttt{trsolve}(L_B^{\top},z)
$$

- $L_B = \texttt{chol}(\Sigma_{\beta}^{-1}+U^{\top}U)$
- $z$ are p independent standard normal variables
- After $M$ iterations, we obtain $$\{\beta^{(1)},\beta^{(2)},\ldots,\beta^{(M)}\}$$, which are samples from $p(\beta\|y)$


#### Random effect $\alpha$

$$
\Sigma_{y|\alpha,\theta} = X\Sigma_{\beta}X^{\top} + D(\theta)
$$

$$
\alpha|\theta,y \sim N(Bb,B)
$$

$$
b = Z(\theta)^{\top}\Sigma_{y|\alpha,\theta}^{-1}(y-X\mu_{\beta})
$$

$$
B = (K(\theta)^{-1} + Z(\theta)^{\top}\Sigma_{y|\alpha,\theta}^{-1}Z(\theta))^{-1}
$$

- $L = \texttt{chol}(\Sigma_{y\|\alpha,\theta^{(k)}})$
- $[v:U]=\texttt{trsolve}(L,[y-X\mu_{\beta}:Z(\theta^{(k)})])$
- $b = U(\theta^{(k)})^{\top}v$

###### Robust computation

$$
G(\theta)^{-1} = Z(\theta)^{\top}\Sigma_{y|\alpha,\theta}^{-1}Z(\theta)
$$

$$
(K(\theta)^{-1}+G(\theta)^{-1})^{-1} = G(\theta) - G(\theta)(K(\theta) + G(\theta))^{-1}G(\theta)
$$

- for current $\theta^{(k)}$, $L = \texttt{chol}(K(\theta^{(k)}) + G(\theta^{(k)}))$, $W = \texttt{trsolve}(L,G(\theta^{(k)}))$
- $L_B=\texttt{chol}(G(\theta^{(k)})-W^{\top}W)$
- let $z$ be a $r\times 1$ vector of independent standard normal variables,
- $a^{(k)} = L_BL_B^{\top}b + L_Bz$
- $$\{\alpha^{(1)},\alpha^{(2)},\ldots,\alpha^{(M)}\}$$ are samples from $p(\alpha\|y)$.

### Low rank models

Let $r \ll n$.

Integrate out $\alpha$,

$$
p(\theta) \times N(\beta|\mu_{\beta},\Sigma_{\beta})\times N(y|X\beta,\Sigma_{y|\beta,\theta})
$$

- $$
\Sigma_{y|\beta,\theta} = Z(\theta)K(\theta)Z(\theta)^{\top} + D(\theta)
$$

- $$
\Sigma_{y|\beta,\theta}^{-1} = D(\theta)^{-1} - D(\theta)^{-1}Z(\theta)(K(\theta)^{-1} + Z(\theta)^{\top}D(\theta)^{-1}Z(\theta))Z(\theta)^{\top}D(\theta)^{-1}
$$

  * $$
\Sigma_{y|\beta,\theta}^{-1} = D(\theta)^{-1/2}(I-H^{\top}H)D(\theta)^{-1/2}
$$

  * $$
H = \texttt{trsolve}(L,W^{\top})
$$

  * $$
W = D(\theta)^{-1/2}Z(\theta)
$$

  * $$
L = \texttt{chol}(K(\theta)^{-1}+W^{\top}W)
$$

  * $$
[v:V] = D^{-1/2}[y:X]
$$

  * $$
\tilde{V} = HV
$$

  * $$
b = \Sigma_{\beta}^{-1}\mu_{\beta} + V^{\top}y - \tilde{V}^{\top}Hv
$$

  * $$
L_B = \texttt{chol}(\Sigma_{\beta}^{-1} + V^{\top}V - \tilde{V}^{\top}\tilde{V})
$$

$$
\log p(\theta|y)= \text{const.} + \log p(\theta) - \frac{1}{2}\log|\Sigma_{y|\beta,\theta}| - \frac{1}{2}Q(\theta)
$$

$$
Q(\theta) = (y-X\beta)^{\top} \Sigma_{y|\beta,\theta}^{-1}(y-X\beta)
$$

$$
v = D(\theta)^{-1/2}(y-X\beta)
$$

$$
w = Hv
$$

$$
T = \texttt{chol}(I_r - HH^{\top})
$$

$$
\log p(\theta) - \frac{1}{2}\sum_{i=1}^n\log d_{i,i}(\theta) + \sum_{i=1}^{n^*}\log t_{i,i}-\frac{1}{2}(v^{\top}v - w^{\top}w)
$$
